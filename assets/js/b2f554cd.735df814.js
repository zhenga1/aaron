"use strict";(self.webpackChunkaaronzheng=self.webpackChunkaaronzheng||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"Anodroid","metadata":{"permalink":"/blog/Anodroid","editUrl":"https://github.com/zhenga1/aaronzheng/tree/main/blog/blog/2022-12-31-Anodroid_blog/Anodroid_blog.mdx","source":"@site/blog/2022-12-31-Anodroid_blog/Anodroid_blog.mdx","title":"Anodroid Blog","description":"","date":"2022-12-31T00:00:00.000Z","formattedDate":"December 31, 2022","tags":[{"label":"anodroid","permalink":"/blog/tags/anodroid"},{"label":"humanoid","permalink":"/blog/tags/humanoid"}],"readingTime":0.46,"hasTruncateMarker":false,"authors":[{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"}],"frontMatter":{"slug":"Anodroid","title":"Anodroid Blog","custom_edit_url":null,"hide_table_of_contents":false,"authors":{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"},"tags":["anodroid","humanoid"]},"nextItem":{"title":"JIPCAD Blog","permalink":"/blog/JIPCAD"}},"content":"import ReactPlayer from \'react-player\'\\r\\nimport Anodroidpic from \'@site/static/Anodroid_demo.mp4\';\\r\\n\\r\\n\\r\\n**Table of Contents:**\\r\\n- Overview\\r\\n- Example Pictures of Robot\\r\\n- My contributions\\r\\n\\r\\n### Overview\\r\\n**Anodroid** is a 12-DOF humanoid-robot of height 54cm(22 inch). It is an integrated machine that can move around in flat and tilted surfaces. \\r\\n\\r\\n### Exemplar Pictures of the Robot\\r\\n![Image](./anodroid_pic.png)\\r\\n\\r\\n<video width=\\"100%\\" height=\\"100%\\" controls muted>\\r\\n     <source src={Anodroidpic}/>\\r\\n</video>\\r\\n\\r\\n### My Contributions\\r\\nThis project was independently designed and built by me as part of a personal project. During the development of *Anodroid*, much inspiration was derived from the [Poppy project](https://www.poppy-project.org/en/)."},{"id":"JIPCAD","metadata":{"permalink":"/blog/JIPCAD","editUrl":"https://github.com/zhenga1/aaronzheng/tree/main/blog/blog/2022-12-31-JIPCAD/JIPCAD_blog.md","source":"@site/blog/2022-12-31-JIPCAD/JIPCAD_blog.md","title":"JIPCAD Blog","description":"UGIS 192D, Fall 2022","date":"2022-12-31T00:00:00.000Z","formattedDate":"December 31, 2022","tags":[{"label":"jipcad","permalink":"/blog/tags/jipcad"},{"label":"nome","permalink":"/blog/tags/nome"}],"readingTime":3.325,"hasTruncateMarker":false,"authors":[{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"}],"frontMatter":{"slug":"JIPCAD","title":"JIPCAD Blog","custom_edit_url":null,"authors":{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"},"hide_table_of_contents":false,"tags":["jipcad","nome"]},"prevItem":{"title":"Anodroid Blog","permalink":"/blog/Anodroid"},"nextItem":{"title":"Phocabulary Blog","permalink":"/blog/Phocabulary"}},"content":"### UGIS 192D, Fall 2022\\r\\n## Aaron Zheng \\r\\n\\r\\n**Table of Contents:**\\r\\n- Overview\\r\\n- Example Photos\\r\\n- My contributions\\r\\n\\t- Development of New VSCode Extension\\r\\n\\t- Identifying bugs and issues\\r\\n- What I learnt\\r\\n\\r\\n## Overview\\r\\n**JIPCAD (Joint Interactive Procedural Computer-Aided Design)** is a new computer-aided design software tool that supports both procedural and interactive modeling. Procedural modeling allows users to use NOME, JIPCAD\'s proprietary programming language to sketch 3D models. On the other hand, users can also use the user interface to model 3D objects, such as by adding new faces and polylines through the interactive program. \\r\\n\\r\\n**JIPCAD** is a design tool that came to being from the **JIPCAD project**, a project initiated and developed by **Professor Carlo Sequin** and his team of researchers at UC Berkeley. **JIPCAD** is specifically built for the modeling of 2-manifold free-form surfaces of high complexity and inherent regularity, like the [Mobius Strip](https://brilliant.org/wiki/mobius-strips/), or sculptures by artists such as [Eva Hild](https://www.evahild.com/ceramics)\xa0or\xa0[Charles O. Perry](http://www.charlesperry.com/).\\r\\n## Example photos of modeling using JIPCAD\\r\\n![Cable Knot](./Nome3_djJ0AXrJiy.png)\\r\\n![Torus](./Nome3_xUyQCC7Llj.png)\\r\\n![Spinal Knot](./Nome3_S36gbJVwhc.png)\\r\\n\\r\\n## My Contributions\\r\\n### Backend support for the NOME Language\\r\\nDuring my time as a researcher for the JIPCAD project, I was able to develop the first version (**0.0.0**) of a [VSCode extension](https://marketplace.visualstudio.com/items?itemName=AaronZheng.nome) for the proprietary NOME JIPCAD language. The features that I managed to include were the following:\\r\\n\\r\\n- **Autocompletion of commands**\\r\\n<img src=\\"https://raw.githubusercontent.com/JIPCAD/JIPCAD-vs-code/master/.github/images/autocompletion.gif\\" />\\r\\n\\r\\nI was able to implement autocompletion features for codeblocks, so that once codeblocks are typed into a code window, the corresponding closing codeblock will appear automatically.  \\r\\n\\r\\n- **Syntax coloring**\\r\\n<img src=\\"https://raw.githubusercontent.com/JIPCAD/JIPCAD-vs-code/master/.github/images/syntax_highlight.png\\" />\\r\\n\\r\\nI was also able to implement *syntax-coloring*, specifically the colouring of **variables**, **commands**, **comments**, and **parameters**. This allows for a more user-friendly interface, as developers using the NOME language can now know what each section of their code represents. \\r\\n\\r\\n- **Commenting**\\r\\n<img src=\\"https://raw.githubusercontent.com/JIPCAD/JIPCAD-vs-code/master/.github/images/toggle_block_comment.gif\\" />\\r\\n\\r\\nThe NOME JIPCAD extension also has the ability of toggling block comments. In addition, with the extension, commenting using the corresponding opening/closing pair `(*` and `*)` is enabled for all files with the `.nom, .jipcad` suffix.\\r\\n\\r\\n- **Running of Nome Executable**\\r\\n<img src=\\"https://raw.githubusercontent.com/JIPCAD/JIPCAD-vs-code/master/.github/images/run_nome.gif\\" />\\r\\n\\r\\nAs demonstrated above, the NOME extension allows developers to run the NOME executable without having to use file explorer or navigate directories terminal. Instead, the extension includes a custom command on VSCode, which allows the NOME executable to be opened and executed. \\r\\n\\r\\n- **Customize directory of NOME executable**\\r\\n<img src=\\"https://raw.githubusercontent.com/JIPCAD/JIPCAD-vs-code/master/.github/images/remote_running.gif\\" />\\r\\nTying back to previous functionality, the NOME VSCode extension allows developers to input a *customised path* of the NOME executable. The default path is the JIPCAD directory located as a subdirectory of the HOME directory. \\r\\n\\r\\nIn the future, I plan to include more advanced features, such as:\\r\\n- **Semantic highlighting**\\r\\n- **Syntax error reporting**\\r\\n\\r\\nI believe these features will significantly improve the experience of developers of the NOME proprietary language, as it will greatly simplify their experience in programming with NOME. \\r\\n\\r\\n### Identifying bugs and issues with the JIPCAD software\\r\\nWhile sketching some designs, I was able to identify some issues in relation to the JIPCAD software. I discovered this whilst trying to sketch a robot using the software.\\r\\n\\r\\n#### My robot sketch\\r\\n![Robot Sketch](./Nome3_cDDVZZUdut.png)\\r\\n\\r\\n**JIPCAD** is able to merge models together. Merging is essential as merging allows the `.nom, .jipcad` design file to be converted into a format compatible with `3D-printing. ` But my design above was not able to be merged. The reason was the inability of JIPCAD to merge two sweeps that share the same face (specifically, the robot\'s shoulder(*green*) and the robot\'s two arms(*yellow*)). \\r\\n\\r\\nAlthough I have discovered the bug, unfortunately I have yet to be able to fix it. \\r\\n\\r\\n### What I learnt\\r\\nDuring the research apprenticeship, I learnt a lot about the NOME proprietary programming language, and how to use it to sketch points, rotated shapes and more. I also obtained a more in-depth understanding of compilers, base-level programming languages like antlr4, regular expressions, using VSCode datasheets like `.json` files, and how to write a grammar for a new programming language."},{"id":"Phocabulary","metadata":{"permalink":"/blog/Phocabulary","editUrl":"https://github.com/zhenga1/aaronzheng/tree/main/blog/blog/2022-12-31-Phocabulary/Phocabulary_blog.mdx","source":"@site/blog/2022-12-31-Phocabulary/Phocabulary_blog.mdx","title":"Phocabulary Blog","description":"","date":"2022-12-31T00:00:00.000Z","formattedDate":"December 31, 2022","tags":[{"label":"phocabulary","permalink":"/blog/tags/phocabulary"}],"readingTime":2.56,"hasTruncateMarker":false,"authors":[{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"}],"frontMatter":{"slug":"Phocabulary","title":"Phocabulary Blog","custom_edit_url":null,"hide_table_of_contents":false,"authors":{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"},"tags":["phocabulary"]},"prevItem":{"title":"JIPCAD Blog","permalink":"/blog/JIPCAD"},"nextItem":{"title":"First Blog Post","permalink":"/blog/first-blog-post"}},"content":"import Phocabularyvid from \'@site/static/Phocabulary_app_demo.mp4\';\\r\\n\\r\\n\\r\\n## **Table of Contents:**\\r\\n- Overview\\r\\n- App Demonstration Video\\r\\n- My contributions\\r\\n\\t- Learning Page\\r\\n\\t- Camera Page\\r\\n\\t- Recycler Interface\\r\\n\\t- AI model and detection\\r\\n- What I learnt\\r\\n\\r\\n## Overview\\r\\n**Phocabulary** is an educational app built for students, by students. Using AI models, Phocabulary allows users to see and learn about physical objects on their camera screens with just a click.\\r\\n\xa0\\r\\nPhocabulary targets children across Hong Kong and not only teaches them vocabulary, but also makes them more aware of their surroundings. Our app is already capable of detecting 90+ objects in the environment. Phocabulary will enable accounts/log-in functionality, allowing users to interact with each other. Users will be able to play fun quizzes with other users, allowing them to retain previously learned knowledge and build friendships.\\r\\n\\r\\n## App Demonstration\\r\\n<video width=\\"100%\\" height=\\"100%\\" controls muted>\\r\\n     <source src={Phocabularyvid}/>\\r\\n</video>\\r\\n\\r\\n## My Contributions\\r\\nIn the Phocabulary project, I was responsible for the development of the application (i.e., making the application idea into a reality) , including all the building of the various pages of the application, features such as the learning, camera, and recycler interface, and most importantly, the AI models that were used to recognise objects within the user\'s camera screen.\\r\\n\\r\\n## Learning Page\\r\\n\\r\\n### Learning with Camera\\r\\n\\r\\n![Camera Learning](./IMG_1755.JPG)\\r\\n\\r\\n### New vocabulary page\\r\\n\\r\\n![New Vocab](./IMG_1745.JPG)\\r\\n\\r\\nThe learning page consists of a custom AlertDialogue interface that pops up when an object in the Camera is being clicked. Once the **Learn More** button gets clicked, the user is being taken to a new Window where they can learn the word in question, see a picture of it, and a definition. If they click the **Got It** button, a Gif of smiling and clapping pops out. \\r\\n\\r\\n## Camera Page:\\r\\n\\r\\n![Camera Page](./IMG_1747.JPG)\\r\\n\\r\\nThis was made with the builtin Android Camera interface.\\r\\n\\r\\n## Recycler Interface:\\r\\n\\r\\n![Recycler Interface](./IMG_1751.JPG)\\r\\n\\r\\nThe recycler interface was built with a dynamically controlled recycler view. The recycler views would be created one at a time, using a datasheet containing all the images and definitions beforehand. \\r\\n\\r\\n## Quiz Interface\\r\\n#### Correct Answer:\\r\\n\\r\\n![Correct Ans](./IMG_1753.JPG)\\r\\n\\r\\n### Wrong Answer:\\r\\n\\r\\n![Wrong Ans](./IMG_1752.JPG)\\r\\n\\r\\n### After Clicking the Show Answer button:\\r\\n\\r\\n![Quiz Interface](./IMG_1746.JPG)\\r\\n\\r\\nThe quiz interface was built with a custom view, comprising of an image on the left, and 3 buttons on the right, representing multiple choice possible answers. Children can click on the buttons to answer the question, which will change the color of the buttons. \\r\\n\\r\\n### AI Models, detection\\r\\n![AI Models](./IMG_1748.JPG)\\r\\n\\r\\nThe AI models used were **SSD-Mobilenet version 2**, an open-sourced artificial intelligence model with object detection capabilities, as well as **You Only Learn Once** (version 5). \\r\\n\\r\\n### What I Learnt\\r\\nI learnt more about how object detection models work, and how to train my own object detection models using transfer learning and opencv. Also gained a much deeper understanding of Android studio and the Android application development api, as well as how to incorporate features such as Camera, how to effectively overlay views (such as rectangular boxes to highlight detection), and how to configure and customise android views."},{"id":"first-blog-post","metadata":{"permalink":"/blog/first-blog-post","editUrl":"https://github.com/zhenga1/aaronzheng/tree/main/blog/blog/2022-12-27-first-blog-post.md","source":"@site/blog/2022-12-27-first-blog-post.md","title":"First Blog Post","description":"Hi internet, this is my first blog post.","date":"2022-12-27T00:00:00.000Z","formattedDate":"December 27, 2022","tags":[{"label":"homepage","permalink":"/blog/tags/homepage"}],"readingTime":0.04,"hasTruncateMarker":false,"authors":[{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"}],"frontMatter":{"slug":"first-blog-post","title":"First Blog Post","authors":{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"},"tags":["homepage"]},"prevItem":{"title":"Phocabulary Blog","permalink":"/blog/Phocabulary"},"nextItem":{"title":"Aquatech Blog","permalink":"/blog/Aquatech"}},"content":"Hi internet, this is my first blog post."},{"id":"Aquatech","metadata":{"permalink":"/blog/Aquatech","editUrl":"https://github.com/zhenga1/aaronzheng/tree/main/blog/blog/2022-3-17-Aquatech/Aquatech_blog.mdx","source":"@site/blog/2022-3-17-Aquatech/Aquatech_blog.mdx","title":"Aquatech Blog","description":"","date":"2022-03-17T00:00:00.000Z","formattedDate":"March 17, 2022","tags":[{"label":"aquatech","permalink":"/blog/tags/aquatech"},{"label":"android apps","permalink":"/blog/tags/android-apps"}],"readingTime":0.9,"hasTruncateMarker":false,"authors":[{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"}],"frontMatter":{"slug":"Aquatech","title":"Aquatech Blog","custom_edit_url":null,"hide_table_of_contents":false,"authors":{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"},"tags":["aquatech","android apps"]},"prevItem":{"title":"First Blog Post","permalink":"/blog/first-blog-post"},"nextItem":{"title":"Zensafety Blog","permalink":"/blog/Zensafety"}},"content":"import Aquatechvid from \'@site/static/Aquatech_app_demo.mp4\';\\r\\n\\r\\n\\r\\n## **Table of Contents:**\\r\\n- Overview\\r\\n- App Demonstration Video\\r\\n- What I learnt\\r\\n\\r\\n## Overview\\r\\n**Aquatech** is a simple environmental protection app which aims to help in the effort of protecting and maintaining Hong Kong\'s beaches. It was made\\r\\nas part of a Hackathon project involving 3 people, myself included. \\r\\n\\r\\n**Aquatech** serves as both a water-quality informational service and a water-quality monitering service. Our app contains initial data on 3 Hong Kong beaches\' water quality, namely Repulse Bay, Ma On Shan, and Mui Wo beach, which our team measured using our sensors which capture conductivity, PH and temperature data. \\r\\nWe also enable users to share data that they collect with others through this app.\\r\\n\\r\\nOur app is available for download here:\\r\\nhttps://play.google.com/store/apps/details?id=comp.envirobros.administrator.aquatech\\r\\n\\r\\n## App Demonstration\\r\\n<video width=\\"100%\\" height=\\"100%\\" controls muted>\\r\\n     <source src={Aquatechvid}/>\\r\\n</video>\\r\\n\\r\\n### What I Learnt\\r\\nI learnt more about developing Android applications. I also learnt about databasing, and how to enable different users to share data with each other. Moreover, I learnt more about UI, UX, and how to make good user interfaces."},{"id":"Zensafety","metadata":{"permalink":"/blog/Zensafety","editUrl":"https://github.com/zhenga1/aaronzheng/tree/main/blog/blog/2022-1-2-Zensafety/2022-1-2-Zensafety_blog.mdx","source":"@site/blog/2022-1-2-Zensafety/2022-1-2-Zensafety_blog.mdx","title":"Zensafety Blog","description":"","date":"2022-01-02T00:00:00.000Z","formattedDate":"January 2, 2022","tags":[{"label":"zensafety","permalink":"/blog/tags/zensafety"},{"label":"zenbo","permalink":"/blog/tags/zenbo"}],"readingTime":1.81,"hasTruncateMarker":false,"authors":[{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"}],"frontMatter":{"slug":"Zensafety","title":"Zensafety Blog","authors":{"name":"Aaron Zheng","title":"College Student","url":"https://github.com/zhenga1","image_url":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg","imageURL":"https://raw.githubusercontent.com/zhenga1/aaronzheng/main/img/pfp.jpeg"},"tags":["zensafety","zenbo"],"hide_table_of_contents":false},"prevItem":{"title":"Aquatech Blog","permalink":"/blog/Aquatech"}},"content":"import Zensafetyvid from \'@site/static/Zenbo_demo.mp4\'\\r\\n\\r\\n## **Table of Contents:**\\r\\n- Overview\\r\\n- App Demonstration Video\\r\\n- What I did\\r\\n\\r\\n## Overview\\r\\nMeet **Zenbo**, \xa0a new robot friend for seniors, kids, and anybody else who wants to invite a real-world version of BB-8 into their own home. **Zenbo**, **Zenbo Junior** are humanoid robots (with wheels attached), made by **ASUS**.\\r\\n![Zenbo Image](./zenbo.png)\\r\\n\\r\\nIt has the ability to talk, move around, and act as a robot friend to kids and seniors.\\r\\n\\r\\n**Zensafety** is an application made to work on **Zenbo**, which can help users secure their most prized possessions. Users can select out of a list of 90 objects to track, and **Zensafety** can keep track of the security status of each and every one of those objects using its object detection technology. \\r\\n\\r\\nOpening Zenbo\u2019s camera feature, users can see what zenbo\u2019s camera is seeing, along with customised coloured rectangles labeling the positions and confidence level (which is: how sure Zenbo is of the object detection) of each detected object in the frame, updated every 200-300ms. \\r\\n\\r\\nA security rating from 1-10 will be given to each selected object based on the amount of the selected object detected and the confidence level for each object detection. If\xa0any object is found to not be secured(no detection or less than 60% confidence), the user will be notified. Additionally, the statuses of each tracked object will be written into text documents saved locally that can be accessed by the user through **Zensafety**.\\r\\n\\r\\n**Zensafety** utilises plenty of Zenbo\u2019s in-built features, such as voice-control, allowing Zenbo to directly communicate with users through system-initiative dialogue and build a good interaction experience. Additionally, **Zensafety** demonstrates multimodalily and displays emotions, such as smiling, when first welcoming users to use **Zensafety**, or loving and shyness when being tapped on the head, which helps it act as a companion to the users.\\r\\n\\r\\n## Zensafety Demonstration Video\\r\\n<video width=\\"100%\\" height=\\"100%\\" controls muted>\\r\\n     <source src={Zensafetyvid}/>\\r\\n</video>\\r\\n\\r\\n## What I did\\r\\nThis project was independently designed and developed by me as part of a yearlong course provided by the *HK Academy for Gifted Education*. I was guided by Dr. Wendy Hui of Lingnan University throughout the length of the project."}]}')}}]);